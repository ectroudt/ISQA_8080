all_aucs[nrow(all_aucs) + 1,] <- auc(roc(local_frame$obs, local_frame[[positive_class]]))
}
roc(census_Training_Data[[response_var]], log_Reg_fit$pred[[positive_class]], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))
reg_PRobs <- log_Reg_fit$pred
view(reg_PRobs)
View(glm_probs)
setwd("~/ISQA_8080/Assignments/Assignment_2/")
census_Data <- read.csv("Census.csv")
# Set the training index that will generate 70-30 split when applied to dataset
train_index <- createDataPartition(y = census_Data[[response_var]], p = 0.7, list = FALSE)
# Create both teh training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[-train_index,]
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " >50K"] <- "Yes"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " <=50K"] <- "No"
positive_class <- "Yes"
negative_class <- "No"
View(census_Training_Data)
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
log_Reg_fit <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, family = binomial, trControl = trControl, metric = 'ROC')
(log_Reg_fit_Results <- log_Reg_fit$results)
all_predictions <- log_Reg_fit$pred
all_aucs <- data.frame('AUC' = double())
for (fold in unique(all_predictions$Resample)){
local_data <- all_predictions %>% filter(Resample == fold)
local_prob <- local_data %>% select(positive_class)
local_true <- local_data %>% select(obs)
# get the AUC value from roc
local_frame <- data.frame(local_true,local_prob)
all_aucs[nrow(all_aucs) + 1,] <- auc(roc(local_frame$obs, local_frame[[positive_class]]))
}
all_aucs
View(log_Reg_fit_Results)
all_aucs
View(all_predictions)
roc(census_Training_Data[[response_var]], all_predictions[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))
roc(census_Training_Data[[response_var]], all_predictions[[positive_class]], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))
?roc
census_Training_Data[[response_var]]
glm_probs[, "Yes"]
# Evaluate both datasets
summary(census_Training_Data)
roc(census_Training_Data[[response_var]], all_predictions[[positive_class]], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
?auc
roc(all_predictions$obs, all_predictions$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
View(log_Reg_fit_Results)
View(dataset)
census_Training_Data$Income
all_predictions$obs
levels(census_Training_Data$Income)[levels(census_Validation_Data$Income) == " >50K"] <- "Yes"
levels(census_Training_Data$Income)[levels(census_Validation_Data$Income) == " <=50K"] <- "No"
log_Reg_fit_Validation <- train(as.formula(model_form) , data = census_Validation_Data, method = model_type, family = binomial, trControl = trControl, metric = 'ROC')
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " >50K"] <- "Yes"
census_Data <- read.csv("Census.csv")
# Set the training index that will generate 70-30 split when applied to dataset
train_index <- createDataPartition(y = census_Data[[response_var]], p = 0.7, list = FALSE)
# Create both teh training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[-train_index,]
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " >50K"] <- "Yes"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " <=50K"] <- "No"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " >50K"] <- "Yes"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " <=50K"] <- "No"
log_Reg_fit_Validation <- train(as.formula(model_form) , data = census_Validation_Data, method = model_type, family = binomial, trControl = trControl, metric = 'ROC')
(log_Reg_fit_Val_Results <- log_Reg_fit_Validation$results)
confusionMatrix(log_Reg_fit_Val_Results, 'average')
(log_Reg_fit_Val_Results <- log_Reg_fit_Validation$results)
confusionMatrix(log_Reg_fit_Val_Results, 'average')
View(log_Reg_fit_Val_Results)
View(census_Validation_Data)
View(census_Training_Data)
View(census_Validation_Data)
log_Reg_fit_Results
View(log_Reg_fit_Val_Results)
summary(census_Validation_Data)
describe(census_Validation_Data)
confusionMatrix(table(log_Reg_fit_Val_Results, 'average')
)
log_Reg_fit_Val_Results <- log_Reg_fit_Validation$results
confusionMatrix(log_Reg_fit_Validation, 'average')
all_predictions <- log_Reg_fit$pred
all_predictions_Val <- log_Reg_fit_Validation$pred
roc(all_predictions_Val$obs, all_predictions_Val$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
(log_Reg_fit_Results <- log_Reg_fit$results)
confusionMatrix(log_Reg_fit, 'average')
all_predictions <- log_Reg_fit$pred
roc(all_predictions$obs, all_predictions$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
(log_Reg_fit_Val_Results <- log_Reg_fit_Validation$results)
confusionMatrix(log_Reg_fit_Validation, 'average')
all_predictions_Val <- log_Reg_fit_Validation$pred
roc(all_predictions_Val$obs, all_predictions_Val$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
model_type <- "lda"
lda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
(lda_fit_Results <- lda_fit_$results)
(lda_fit_Results <- lda_fit_$results)
confusionMatrix(lda_fit, 'average')
confusionMatrix(lda_fit_, 'average')
View(lda_fit_Results)
all_predictions_lda <- lda_fit_$pred
roc(all_predictions_lda$obs, all_predictions_lda$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
lda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
(lda_fit_Results <- lda_fit_$results)
confusionMatrix(lda_fit_, 'average')
all_predictions_lda <- lda_fit_$pred
roc(all_predictions_lda$obs, all_predictions_lda$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
lda_fit_Val <- train(as.formula(model_form) , data = census_Validation_Data, method = model_type, trControl = trControl, metric = 'ROC')
(lda_fit_Results_Val <- lda_fit_Val$results)
confusionMatrix(lda_fit_Val, 'average')
all_predictions_lda_Val <- lda_fit_Val$pred
roc(all_predictions_lda_Val$obs, all_predictions_lda_Val$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
View(all_predictions_lda_Val)
model_type <- "qda"
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
view(census_Training_Data)
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
qda_fit_Val <- train(as.formula(model_form) , data = census_Validation_Data, method = model_type, trControl = trControl, metric = 'ROC')
trControl <- trainControl(method = 'cv', number = 5, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
qda_fit_Val <- train(as.formula(model_form) , data = census_Validation_Data, method = model_type, trControl = trControl, metric = 'ROC')
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE,  savePredictions = "all", summaryFunction = twoClassSummary)
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions = "all")
trControl <- trainControl(method = 'cv', number = 10, savePredictions = "all", classProbs = TRUE, summaryFunction = twoClassSummary)
qda_fit_Val <- train(as.formula(model_form) , data = census_Validation_Data, method = model_type, trControl = trControl, metric = 'ROC')
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
set.seed(17)
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
library(pROC)
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
head(census_Training_Data)
summary(heart)
describe(heart)
describe(census_Training_Data)
qda_fit_Val <- train(as.formula(model_form) , data = census_Validation_Data, method = model_type, trControl = trControl)
model_form <- Income ~ . -WeeklyHours
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
model_form <- Income ~ . -WeeklyHours -Sex
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
model_form <- Income ~ .
qda_fit_ <- train(as.formula(model_form) , data = census_Data, method = model_type, trControl = trControl, metric = 'ROC')
setwd("~/ISQA_8080/Assignments/Assignment_2/")
census_Data <- read.csv("Census.csv")
set.seed(1)
# Set the training index that will generate 70-30 split when applied to dataset
train_index <- createDataPartition(y = census_Data[[response_var]], p = 0.7, list = FALSE)
# Create both teh training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[-train_index,]
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " >50K"] <- "Yes"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " <=50K"] <- "No"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " >50K"] <- "Yes"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " <=50K"] <- "No"
model_type <- "qda"
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
?train
?trControl
?trainControl
library(MASS)
model_type <- "qda"
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
detach("package:magrittr", unload=TRUE)
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
#model_type <- "qda"
model_form <- Income ~ WeeklyHours + Age + Occupation
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
#model_type <- "qda"
model_form <- Income ~ WeeklyHours + Age
model_type <- "qda"
model_form <- Income ~ WeeklyHours + Age
qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
(qda_fit_Results <- qda_fit_$results)
confusionMatrix(qda_fit_, 'average')
qda_fit_Val <- train(as.formula(model_form) , data = census_Validation_Data, method = model_type, trControl = trControl, metric = 'ROC')
# note: we can specify the formula like this. if you specify individual predictors, they have to match the column names in the dataset
model_form <- AHD ~ Age + Sex + RestBP + Chol + Ca
model_type <- "glm"
positive_class <- "Yes"
negative_class <- "No"
heart <- read_csv("~/ISQA_8080/Week_4_Classification/Heart.csv")
# note that the first column is only the observation index. we can delete this
dataset <- prepare_heart(heart)
source("./Utils.R")
set.seed(17)
target_var <- 'AHD'
detach("package:MASS", unload=TRUE)
# note: we can specify the formula like this. if you specify individual predictors, they have to match the column names in the dataset
model_form <- AHD ~ Age + Sex + RestBP + Chol + Ca
# note that the first column is only the observation index. we can delete this
dataset <- prepare_heart(heart)
glm_fit <- train(as.formula(model_form) , data = dataset, method = model_type, family = binomial, trControl = trControl, metric = 'ROC') # the metric parameter indicates which metric the cross validation will focus on
# we can get the results of the resampling, averaged over the k folds, as follows:
glm_fit_cv_results <- glm_fit$results
# if we want some additional information, we can use following parameters
glm_fit$resample # gives you the average and kappa statistic for each fold
# if we want some additional information, we can use following parameters
glm_fit$resample # gives you the average and kappa statistic for each fold
target_var <- 'AHD'
threshold <- 0.5
model_type <- "knn"
model_form <- AHD ~ Age + Sex + RestBP + Chol + Ca
dataset$Ca <- factor(dataset$Ca, levels = c(levels(dataset$Ca),"-1")) # add an additional factor level
dataset$Ca[is.na(levels(dataset$Ca)[dataset$Ca])] <- "-1"
# let's build an initial model with k = 3
tuneGrid <- expand.grid(k = 3) # note: a grid search usually considers a set of parameters, not only one. However, in the current implementation of caret only one parameter is allowed when we don't use resampling
knn_fit <- train(as.formula(model_form), data = dataset, method = model_type, trControl = trControl, tuneGrid = tuneGrid)
# sidenote: if you want to automatically loop through multiple values of k, you need to delete the trControl parameter (courtesy of caret)
tuneGrid <- expand.grid(k = 3:7)
View(tuneGrid)
knn_fit <- train(as.formula(model_form), data = dataset, method = model_type, tuneGrid = tuneGrid)
knn_fit # note: the accuracy values shown here are much lower than the ones before, mostly because caret uses an implicit bootstrapping resampling to estimate the predicted accuracy
# let's build an initial model with k = 3
tuneGrid <- expand.grid(k = 3) # note: a grid search usually considers a set of parameters, not only one. However, in the current implementation of caret only one parameter is allowed when we don't use resampling
knn_fit <- train(as.formula(model_form), data = dataset, method = model_type, trControl = trControl, tuneGrid = tuneGrid)
knn_fit # note: the accuracy values shown here are much lower than the ones before, mostly because caret uses an implicit bootstrapping resampling to estimate the predicted accuracy
# sidenote: if you want to automatically loop through multiple values of k, you need to delete the trControl parameter (courtesy of caret)
tuneGrid <- expand.grid(k = 3:7)
knn_fit <- train(as.formula(model_form), data = dataset, method = model_type, tuneGrid = tuneGrid)
set.seed(1)
response_var <- 'Income'
# Set the training index that will generate 70-30 split when applied to dataset
train_index <- createDataPartition(y = census_Data[[response_var]], p = 0.7, list = FALSE)
# Create both teh training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[-train_index,]
# Recreate boilerplate code from classification lecture
model_form <- Income ~ .
model_type <- "glm"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " >50K"] <- "Yes"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " <=50K"] <- "No"
positive_class <- "Yes"
negative_class <- "No"
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
log_Reg_fit <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, family = binomial, trControl = trControl, metric = 'ROC')
(log_Reg_fit_Results <- log_Reg_fit$results)
confusionMatrix(log_Reg_fit, 'average')
all_predictions <- log_Reg_fit$pred
roc(all_predictions$obs, all_predictions$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
model_form <- AHD ~ Age + Sex + RestBP + Chol + Ca
target_var <- "AHD"
model_type <- 'lda'
trControl <- trainControl(method='none')
# with the lda method, we can can specify the model just as we did with the glm function
lda_fit <- train(as.formula(model_form), data = dataset, method = model_type, trControl = trControl)
# the lda_fit object gives us some information about prior probabilities and coefficients of the linear discriminants
lda_fit$finalModel
# we can also use the predict function again to predict new outcomes
lda_class <- lda_fit %>%  predict(type ='raw') # this assumes a threshold of 0.5
lda_probs <- lda_fit %>% predict(type = 'prob')
confusionMatrix(lda_class, dataset[[target_var]], positive = positive_class)
confusionMatrix(lda_class, dataset[[target_var]], positive = positive_class)
View(lda_probs)
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " >50K"] <- "Yes"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " <=50K"] <- "No"
# Recreate boilerplate code from classification lecture
model_form <- Income ~ .
model_type <- "glm"
positive_class <- "Yes"
negative_class <- "No"
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
log_Fit_Val_Probs <- log_Reg_fit %>% predict(newdata = census_Validation_Data, type = 'prob')
View(log_Fit_Val_Probs)
log_Fit_Val_Raw <- log_Reg_fit %>% predict(newdata = census_Validation_Data, type = 'raw')
confusionMatrix(log_Fit_Val_Raw, census_Validation_Data[[response_var]], positive = positive_class)
log_Fit_Val_Raw[, positive_class]
log_Fit_Val_Raw[[positive_class]]
roc(census_Validation_Data[[response_var]], log_Fit_Val_Probs[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))
lda_fit_Val_Raw <- lda_fit_ %>% predict(newdata = census_Validation_Data, type = 'raw')
lda_fit_Val_Probs <- lda_fit_ %>% predict(newdata = census_Validation_Data, type = 'prob')
confusionMatrix(lda_fit_Val_Raw, census_Validation_Data[[response_var]], positive = positive_class)
roc(census_Validation_Data[[response_var]], lda_fit_Val_Probs[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))
model_type <- "lda"
lda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
all_predictions_lda <- lda_fit_$pred
roc(all_predictions_lda$obs, all_predictions_lda$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
qda_fit_Val_Raw <- qda_fit_ %>% predict(newdata = census_Validation_Data, type = 'raw')
qda_fit_Val_Probs <- qda_fit_ %>% predict(newdata = census_Validation_Data, type = 'prob')
confusionMatrix(qda_fit_Val_Raw, census_Validation_Data[[response_var]], positive = positive_class)
roc(census_Validation_Data[[response_var]], qda_fit_Val_Probs[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))
model_type <- "knn"
threshold <- 0.5
tuneGrid <- expand.grid(k = 3:7)
knn_fit <- train(as.formula(model_form), data = census_Training_Data, method = model_type, trControl = trControl, tuneGrid = tuneGrid)
(knn_fit_Results <- knn_fit$results)
confusionMatrix(knn_fit, 'average')
all_predictions_knn <- knn_fit$pred
roc(all_predictions_knn$obs, all_predictions_knn$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
knn_fit_Val_Raw <- knn_fit %>% predict(newdata = census_Validation_Data, type = 'raw')
qda_fit_Val_Probs <- knn_fit %>% predict(newdata = census_Validation_Data, type = 'prob')
confusionMatrix(knn_fit_Val_Raw, census_Validation_Data[[response_var]], positive = positive_class)
qda_fit_Val_Probs <- knn_fit %>% predict(newdata = census_Validation_Data, type = 'prob')
knn_fit_Val_Probs <- knn_fit %>% predict(newdata = census_Validation_Data, type = 'prob')
roc(census_Validation_Data[[response_var]], knn_fit_Val_Probs[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))
library("magrittr", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.6")
?predict
model_form <- AHD ~ Age + Sex + RestBP + Chol + Ca
target_var <- "AHD"
model_type <- 'lda'
positive_class <- 'Yes'
summary(lda_class)
summary(lda_probs)
model_type <- "glm"
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
log_Reg_fit <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, family = binomial, trControl = trControl, metric = 'ROC')
# Recreate boilerplate code from classification lecture
model_form <- Income ~ .
model_type <- "glm"
set.seed(1)
response_var <- 'Income'
# Set the training index that will generate 70-30 split when applied to dataset
train_index <- createDataPartition(y = census_Data[[response_var]], p = 0.7, list = FALSE)
# Create both teh training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[-train_index,]
# Recreate boilerplate code from classification lecture
model_form <- Income ~ .
model_type <- "glm"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " >50K"] <- "Yes"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " <=50K"] <- "No"
positive_class <- "Yes"
negative_class <- "No"
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
log_Reg_fit <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, family = binomial, trControl = trControl, metric = 'ROC')
confusionMatrix(log_Reg_fit, 'average')
(log_Reg_fit_Results <- log_Reg_fit$results)
confusionMatrix(log_Reg_fit, 'average')
roc(all_predictions$obs, all_predictions$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " >50K"] <- "Yes"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " <=50K"] <- "No"
log_Fit_Val_Raw <- log_Reg_fit %>% predict(newdata = census_Validation_Data, type = 'raw')
log_Fit_Val_Probs <- log_Reg_fit %>% predict(newdata = census_Validation_Data, type = 'prob')
confusionMatrix(log_Fit_Val_Raw, census_Validation_Data[[response_var]], positive = positive_class)
(qda_fit_Results <- qda_fit_$results)
all_predictions_qda <- qda_fit_$pred
View(lda_fit_Results)
heart <- read_csv("~/ISQA_8080/Week_4_Classification/Heart.csv")
# note that the first column is only the observation index. we can delete this
heart <- heart %>% select(-c(1))
head(heart)
# we can look at scatterplots and pairwise comparisons first
ggpairs(heart, aes(color = AHD))
head(heart)
# as we can see, some variables are actually factors, not numerical
heart <- heart %>% mutate_at(c("Sex","Fbs","RestECG","ExAng","Slope","Ca"), as.factor)
head(heart)
head(census_Data)
setwd("~/ISQA_8080/Assignments/Assignment_2/")
census_Data <- read.csv("Census.csv")
head(census_Data)
?sapply
sapply(census_Data, class)
census_Data <- read.csv("Census.csv")
response_var = "Income"
head(census_Data)
head(census_Data)
head(census_Data, n = 10)
census_Data[[response_var]] <- relevel(census_Data[[response_var]], positive_class)
census_Data[[response_var]] <- relevel(census_Data[[response_var]], " >50K")
head(census_Data, n = 10)
set.seed(1)
# Set the training index that will generate 70-30 split when applied to dataset
train_index <- createDataPartition(y = census_Data[[response_var]], p = 0.7, list = FALSE)
# Create both teh training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[-train_index,]
# Evaluate both datasets
summary(census_Training_Data)
summary(census_Validation_Data)
# Recreate boilerplate code from classification lecture
model_form <- Income ~ .
model_type <- "glm"
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
log_Reg_fit <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, family = binomial, trControl = trControl, metric = 'ROC')
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " >50K"] <- "Yes"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " <=50K"] <- "No"
positive_class <- "Yes"
negative_class <- "No"
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
log_Reg_fit <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, family = binomial, trControl = trControl, metric = 'ROC')
(log_Reg_fit_Results <- log_Reg_fit$results)
confusionMatrix(log_Reg_fit, 'average')
# Create both teh training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[,train_index]
census_Validation_Data <- census_Data[,-train_index]
census_Validation_Data <- census_Data[-train_index,]
set.seed(1)
# Set the training index that will generate 70-30 split when applied to dataset
train_index <- createDataPartition(y = census_Data[[response_var]], p = 0.7, list = FALSE)
# Create both teh training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[-train_index,]
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " >50K"] <- "Yes"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " <=50K"] <- "No"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " >50K"] <- "Yes"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " <=50K"] <- "No"
model_type <- "lda"
lda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
(lda_fit_Results <- lda_fit_$results)
set.seed(2)
# Set the training index that will generate 70-30 split when applied to dataset
train_index <- createDataPartition(y = census_Data[[response_var]], p = 0.7, list = FALSE)
# Create both teh training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[-train_index,]
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " >50K"] <- "Yes"
levels(census_Training_Data$Income)[levels(census_Training_Data$Income) == " <=50K"] <- "No"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " >50K"] <- "Yes"
levels(census_Validation_Data$Income)[levels(census_Validation_Data$Income) == " <=50K"] <- "No"
model_type <- "lda"
lda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')
(lda_fit_Results <- lda_fit_$results)
setwd("~/ISQA_8080/Week_6_Decision_Trees/")
install.packages("doParallel")
# new: let's parallelize things. thanks to the caret package, we only need the following four lines to fully parallelize the model training
library(doParallel)
num_cores <- detectCores() #note: you can specify a smaller number if you want
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
library(randomForest)
install.packages("randomForest")
library(randomForest)
library(pROC)
library(caret)
library(tidyverse)
# set your working directory to the current file directory
tryCatch({
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}, error=function(cond){message(paste("cannot change working directory"))
})
# first, let's load the util functions
source("../Utils.R")
# first, let's load the util functions
source("../Week_4_Classification/Utils.R")
set.seed(1)
# let's use the heart data set again
heart <- read_csv("Heart.csv")
dataset <- NULL
dataset <- prepare_heart(heart)
target_var <- 'AHD'
# note: we can specify the formula like this. if you specify individual predictors, they have to match the column names in the dataset
model_form <- AHD ~ Age + Sex + RestBP + Chol + Ca + Thal
model_type <- "rf"
positive_class <- "Yes"
negative_class <- "No"
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]
# we will use a 10-fold cross validation on the training set
trControl <- trainControl(method = 'cv', savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
rf_heart <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'ROC')
rf_heart
# textual summary of the outcome
rf_heart$finalModel
confusionMatrix(rf_heart)
rf_heart
?trainControl
# for variable importance, we can look into following functions
importance(rf_heart$finalModel)
varImpPlot(rf_heart$finalModel)
# let's predict its performance on the validation / test data
rf_heart_pred <- rf_heart %>% predict(newdata = data_test, type = 'raw')
confusionMatrix(rf_heart_pred, data_test[[target_var]], positive = positive_class)
# alternatively, check other thresholds
threshold <- 0.4
expand.grid(cp=c(0.0))
expand.grid(cp=c(1.0))
expand.grid(cp=c(1.1))
expand.grid(cp=c(2))
expand.grid(cp=c(2,2))
expand.grid(cp=c(5,5))
?train
titanic <- read_csv("titanic.csv")# read data
dataset <- NULL
dataset <- prepare_titanic(titanic)
target_var <- 'survived'
model_form <- survived ~ sex + age + sibsp + parch + fare + embarked
positive_class = 'Yes'
negative_class = 'No'
model_type <- 'rf'
trainIndex_HO <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
data_train_HO <- dataset[trainIndex,]
data_test_HO <- dataset[-trainIndex,]
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary, search = 'random')
rf_titanic_random <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'ROC', tuneLength = 20)
set.seed(1)
target_var <- 'survived'
# note: we can specify the formula like this. if you specify individual predictors, they have to match the column names in the dataset
model_form <- survived ~ sex + age + sibsp + parch + embarked
model_type <- "rpart"
positive_class <- "Yes"
negative_class <- "No"
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]
# we will use a 10-fold cross validation on the training set to select the best complexity parameter
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE)
titanic <- read_csv("titanic.csv")# read data
dataset <- NULL
dataset <- prepare_titanic(titanic)
target_var <- 'survived'
model_form <- survived ~ sex + age + sibsp + parch + fare + embarked
positive_class = 'Yes'
negative_class = 'No'
model_type <- 'rf'
trainIndex_HO <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
data_train_HO <- dataset[trainIndex,]
data_test_HO <- dataset[-trainIndex,]
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary, search = 'random')
rf_titanic_random <- train(as.formula(model_form), data = data_train_HO, method = model_type, trControl = trControl, metric = 'ROC', tuneLength = 20)
rf_titanic_random
rf_titanic_random_pred <- rf_titanic_random %>% predict(newdata = data_test_HO, type = 'raw')
confusionMatrix(rf_titanic_random_pred, data_test_HO[[target_var]], positive = positive_class)
(.mtry = c(1:10))
importance(rf_titanic_random$finalModel)
