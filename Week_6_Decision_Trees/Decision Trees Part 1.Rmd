---
title: "Decision Trees"
author: "Dr. Christian Haas"
date: "September 20, 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

This section covers various decision tree methodologies.

# Regression Trees

Let's start with the Regression Tree example first. Regression trees divide the space into distinct, non-overlapping regions, and use the average Y value in the region to make a prediction.

Note: R has many different packages for decision trees available, e.g., tree, party, or rpart. They mostly work with different implementations and splitting rules. For our purposes, we're going to use rpart.

```{r regression-trees}
# set your working directory to the current file directory 
tryCatch({
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
  }, error=function(cond){message(paste("cannot change working directory"))
})

# first, let's load the util functions
source("../Week_5_Resampling/Utils.R")

# Fitting Regression Trees

library(ISLR)
#library(party) # for the decision trees
library(caret)
library(rpart)
library(rpart.plot)
library(tidyverse)

# load the data set in memory
hitters <- Hitters 
# remove the rows with the NAs in the target variable
dataset <- NULL
dataset <- hitters %>% drop_na(Salary)

# specify the model
target_var <- "Salary"
model_form <- log(Salary) ~ Years + Hits
model_type <- 'rpart' # for decision trees

# divide into training and test
set.seed(12)
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]

# we will use a 10-fold cross validation on the training set to select the best complexity parameter
trControl <- trainControl(method = 'cv', number = 10)
# set the tuneGrid parameter
tGrid <- expand.grid(cp=c(0.0)) # this is the complexity parameter that specifies how complex the decision tree should be

# let's build a regression tree. Note that cp = 0.00 means we build a tree with a maximum complexity.
# we can use a more elaborate tuneGrid grid search with cross validation to find the best tree complexity later
tree <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, tuneGrid = tGrid)
# textual summary of the tree
summary(tree)
# let's visualize the tree. Note that extra = 1 here means that the percentage of observations that fall under a certain split are shown for each leaf.
# for classification trees, we can extend this to show the misclassification rate per node
rpart.plot(tree$finalModel, type = 1)

# now, let's find the best complexity parameter via training and internal cross-validation
# note: if we don't specify specific parameters using tuneGrid, we can do a random search for the parameters using tuneLength
tree_opt <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, tuneLength = 3)
tree_opt

# plot the tree. 
rpart.plot(tree_opt$finalModel, type = 1, extra = 1)

# predict on the test/validation set, once without, once with tree pruning
tree_pred <- tree %>% predict(newdata = data_test, type = 'raw')
tree_prune_pred <- tree_opt %>% predict(newdata = data_test, type = 'raw')

# calculate the MSEs and Rsquareds
postResample(tree_pred, log(data_test[[target_var]]))
postResample(tree_prune_pred, log(data_test[[target_var]]))

# in this case, the RMSE is indeed smaller for the optimized tree

```


## Classification Trees

```{r classification-trees}
# Fitting Classification Trees

source("../Utils.R")
library(rpart)
library(rpart.plot)
library(ISLR)
library(tidyverse)
library(caret)

heart <- read_csv("Heart.csv")
dataset <- NULL
dataset <- prepare_heart(heart)

set.seed(1)

target_var <- 'AHD'
# note: we can specify the formula like this. if you specify individual predictors, they have to match the column names in the dataset
model_form <- AHD ~ Age + Sex + RestBP + Chol + Ca
model_type <- "rpart"
positive_class <- "Yes"
negative_class <- "No"

trainIndex <- createDataPartition(dataset[[target_var]], p = 0.7, list = FALSE)
data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]

# we will use a 10-fold cross validation on the training set to select the best complexity parameter
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE)
tGrid <- expand.grid(cp=c(0.0))

tree_heart <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'Accuracy', tuneGrid = tGrid)
# textual summary of the tree
tree_heart
summary(tree_heart)
rpart.plot(tree_heart$finalModel, type = 1, extra = 1, under = TRUE)

# let's see how this changes with a different complexity parameter
tGrid <- expand.grid(cp=c(0.01))

tree_heart_alt <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'Accuracy', tuneGrid = tGrid)
tree_heart_alt
rpart.plot(tree_heart_alt$finalModel, type = 1, extra = 1, under = TRUE)

# for classification trees, extra = 2 displays the sclassification rate
rpart.plot(tree_heart_alt$finalModel, type = 1, extra = 2, under = TRUE)

# let's predict its performance on the validation / test data
tree_heart_pred = tree_heart %>% predict(newdata = data_test, type = 'raw')

confusionMatrix(tree_heart_pred, data_test[[target_var]], positive = positive_class)

# let's determine the best level of pruning, i.e., tree complexity
# the following function gets the best 10-fold cross validation error for alpha (see slides) and builds the tree with the respective complexity
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)

tree_heart_prune <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'ROC', tuneLength = 10)
# a note on tuneLength: we can either specify a fixed parameter 'grid', i.e., give potential values for each parameter that we want to evaluate and then test all combinations, or we can use a random parameter sampling. in this case, the tuneLength parameter indicates how many different random settings (in this case for the complexity parameter cp) are tried

# textual summary of the tree
summary(tree_heart_prune)
plot(tree_heart_prune) # plot the metric based on the complexity parameter
rpart.plot(tree_heart_prune$finalModel, type = 1, extra = 1, under = TRUE)

# let's predict its performance on the validation / test data
tree_heart_prune_pred = tree_heart_prune %>% predict(newdata = data_test, type = 'raw')
confusionMatrix(tree_heart_prune_pred, data_test[[target_var]], positive = positive_class)

```


## Hands-on Session 1

Let's use a decision tree (classification tree) to build a model for the titanic data set.

Use a split between training and test (or cross validation), try different parameters, and see how well you can predict whether a passenger survives or not.
In addition, see if there is a difference in tree pruning or not.

Finally, print the tree (or pruned tree) to see how the tree splits the predictor variables, and which predictor variables are actually important for predicting survival.

```{r hands-on1}
# first, let's load the util functions
source("../Utils.R")

titanic <- read_csv("../Week_4_Classification/titanic.csv")# read data
dataset <- NULL
dataset <- prepare_titanic(titanic)
  
set.seed(1)

target_var <- 'survived'
# note: we can specify the formula like this. if you specify individual predictors, they have to match the column names in the dataset
model_form <- survived ~ sex + age + sibsp + parch + embarked
model_type <- "rpart"
positive_class <- "Yes"
negative_class <- "No"

trainIndex <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]

# we will use a 10-fold cross validation on the training set to select the best complexity parameter
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE)
tGrid <- expand.grid(cp=c(0.0))

FullTree_titanic <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'Accuracy', tuneGrid = tGrid)
# textual summary of the tree

summary(FullTree_titanic)
rpart.plot(FullTree_titanic$finalModel, type = 1, extra = 1, under = TRUE)

# let's see how this changes with a different complexity parameter
tGrid <- expand.grid(cp=c(0.01))

Titanic_alt <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'Accuracy', tuneGrid = tGrid)
tree_heart_alt
rpart.plot(Titanic_alt$finalModel, type = 1, extra = 1, under = TRUE)

# for classification trees, extra = 2 displays the sclassification rate
rpart.plot(Titanic_alt$finalModel, type = 1, extra = 2, under = TRUE)

# let's predict its performance on the validation / test data
FullTree_titanic_pred = FullTree_titanic %>% predict(newdata = data_test, type = 'raw')

confusionMatrix(FullTree_titanic_pred, data_test[[target_var]], positive = positive_class)

Titanic_alt_pred = Titanic_alt %>% predict(newdata = data_test, type = 'raw')

confusionMatrix(Titanic_alt_pred, data_test[[target_var]], positive = positive_class)



# let's determine the best level of pruning, i.e., tree complexity
# the following function gets the best 10-fold cross validation error for alpha (see slides) and builds the tree with the respective complexity
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)

PrunedTree_titanic <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'ROC', tuneLength = 20)
# a note on tuneLength: we can either specify a fixed parameter 'grid', i.e., give potential values for each parameter that we want to evaluate and then test all combinations, or we can use a random parameter sampling. in this case, the tuneLength parameter indicates how many different random settings (in this case for the complexity parameter cp) are tried

# textual summary of the tree

plot(PrunedTree_titanic) # plot the metric based on the complexity parameter
rpart.plot(PrunedTree_titanic$finalModel, type = 1, extra = 1, under = TRUE)

# let's predict its performance on the validation / test data
PrunedTree_titanic_pred = PrunedTree_titanic %>% predict(newdata = data_test, type = 'raw')
confusionMatrix(PrunedTree_titanic_pred, data_test[[target_var]], positive = positive_class)





```

