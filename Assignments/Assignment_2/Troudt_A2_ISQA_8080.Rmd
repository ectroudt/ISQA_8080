---
title: "ISQA_8080 Assignment 2"
author: "Eric Troudt"
date: "October 5, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

##__Question 1__

**a.** To determine the average number of observations occurring within an interval of any given value of X, where the interval is 10% of the range of X, the probability of X occurring within that interval must be computed. Since X follows a normal distribution, the area under the curve within the given interval must be computed using either z or t scores. If x is 0.6, then P(0.55 <= X <= 0.65) is the fraction of available observations that can be used for predicting the response value. Multiplying by the total number of observations (n) will provide the average number of values that fall within that interval. 

**b.** If the same question is applied with p = 2, where the additional predictor is also normally distributed, then the same calculation will apply for the interval in X2. If X2 = 0.35, then P(0.30 <= X2<= 0.40) is the fraction of available X2 observations that can be used for predicting the response value. To get the probability of both predictors X1 and X2 occurring within their respective intervals, simply multiply the probability for X1 (from a.) by the probability of X2. This overall probaility can then be multiplied by the total number of observations (n) to determine the number of observations on average that will occur with both X1 and X2 residing in their intervals.

**c**. For the same question, in order to compute the overall probability of each predictor variable occurring within its specified interval, each individual probability must be multiplied together. If p = 5, then Ptotal = P(X1) * P(X2) * P(X3) * P(X4) * P(X5), and likewise this overall probaility can then be multiplied by the total number of observations (n) to determine the number of observations on average that will occur with the p variables residing in their intervals.

**d**. As can be seen, the more predictor variables there are, the smaller the probability of having observations in which each variable occurs within its specified interval. For the KNN approach, this means fewer neighbors (if any at all) within the requisite distance of x0, leading to a drastically decreased model performance once p becomes prohibitively large. The textbook describes this as the ‘curse of dimensionality’ (section 3.5), and emphasizes the use of parametric models when p is large or if there are relatively few observations for each predictor variable.

##__Question 2__

**a**. The sample is split into k equal groups where all but one group are used as training data fitted by the statistical learning method. The excluded group is then utilized as the test data upon which the statistical model is applied, with the prediction error serving as a measure of the test error rate. K-fold CV carries out this process iteratively so that each of the k groups is held out as the test group once around, resulting in k error rates that are then averaged to produce the actual estimate of the test error rate.

**b**. _(Answers based on textbook sections 5.1.2 - 5.1.4)_
  
* *The validation set approach?* 
	  
	  + __Advantages__: K-fold CV will incorporate far more observations from the sample in the numerous fitting iterations that take place (each fitting process occurs on all observations except those curently being held out as the test data). This provides for less biased estimates of the test error rate compared with validation-set approach, where a much smaller portion of the observations are included in the training data used for fitting, resulting in reduced model performance with relatively larger error rates. Additionally, each of the error rate estimations given by the validation-set approach can exhibit considerable variance for a given sample, since the training data is comprised of a randomly selected portion of the observations that can differ significantly each time the approach is applied.
	  + __Disadvantages__: Depending on the complexity of the model being used, along with the corresponding sample size, there may be additional computational expense with the k-fold CV approach, since it involves k instances of implementing the model, whereas set-validation only requires one iteration.     
	  
* *LOOCV?*   
	
	  + __Advantages__: The computational expense associated with LOOCV (n model implementations) is much greater compared with the k implementations carried out using k-fold CV, and therefore k-fold CV will be much less demanding on the computing resources being used The degree of overlap between the different training data-sets used in k-fold CV is less pronounced when compared with the training data-sets applied in LOOCV, all of which are highly similar, only differing by one value (the test data). The high degree of similarity between the n training sets in LOOCV produce highly correlated estimates of the test error rate, resulting in an overall estimate that has higher variance compared with estimate generated using k-fold CV.
    + __Disadvantages__:
	While the error rate estimate from k-fold CV has less variance than LOOCV, it also has more bias, since its training data-sets are comprised of a relatively smaller fraction of the sample compared with LOOCV. 
**Choosing a k-value of 5 or 10 is recommended for effectively balancing out the bias-variance trade-off of the error-rate estimate.

**c**.
The advantages of using cross-validation on the entire data-set include an estimate of the error-rate that will have relatively low variance and will only require k implementations of each model tested. The generation of an error-rate curve across the different models being tested (and potentially at different levels of k) will allow for selection of the model with a given parameter setting that produces the minimal error rate.

If the sample data is split 80/20 training:test, with cross validation being used on the training data to select the model, it will allow for a ‘fine-tuning’ of the parameters being considered. This leads the estimation of an error-rate that is based on a set of observations that were not involved at all in fitting the model (20% of the split), and therefore will provide a more valid estimate of that model's actual performance.

##__Question 3__

```{r code_part_a, include=FALSE, message=FALSE}
library(GGally)
library(caret)
library(Hmisc)
library(magrittr)
library(pROC)

setwd("~/ISQA_8080/Assignments/Assignment_2/")

census_Data <- read.csv("Census.csv")

response_var = "Income"
head(census_Data)

census_Data <- census_Data %>% dplyr::mutate(Income, Income = dplyr::recode(Income, " <=50K" = "No", " >50K" = "Yes"))

census_Data[["Income"]] <-relevel(census_Data[["Income"]], "Yes")

# Inspect dataset, are all variables correctly set as factors? How are total? Are there any missing?
names(census_Data)
head(census_Data)
dim(census_Data)
describe(census_Data)

# get pairwise comparisons of income and each predictor variable 
ggpairs(census_Data, columns = c(1, 7), aes(color = Income))
ggpairs(census_Data, columns = c(2, 7), aes(color = Income))
ggpairs(census_Data, columns = c(3, 7), aes(color = Income))
ggpairs(census_Data, columns = c(4, 7), aes(color = Income))
ggpairs(census_Data, columns = c(5, 7), aes(color = Income))
ggpairs(census_Data, columns = c(6, 7), aes(color = Income))


```
###**a**. 

Based off of the pairwise comparisons of income with each predictor variable, marital status appears to be most relevant, with both occupation and sex also having some relevancy to a lesser extent. In all three of these variables, there are certain levels that exhibit a much larger proportion of income > 50K compared with the other levels, making them relevant for predicting whether an individual’s income is > 50K.

```{r code_part_b, message=FALSE,  warning=FALSE, echo=FALSE}

set.seed(1)


# Set the training index that will generate 70-30 split when applied to dataset
train_index <- createDataPartition(y = census_Data[[response_var]], p = 0.7, list = FALSE)

# Create both the training and validation datasets
census_Training_Data <- census_Data[train_index,]
census_Validation_Data <- census_Data[-train_index,]


# Evaluate both datasets
summary(census_Training_Data)
summary(census_Validation_Data)
```

###**b**. 

After generating the 70-30 split and producing summaries of each data set, the distributions of the variables within each data-set appear to be very similar, without any noticeable significant differences.

###**c**.

####_Results from using K-fold CV with k=10 on a logistic regression model for Census training data ("<=50K" = Yes, ">50K" = No):_


```{r code_part_c_1, message=FALSE,  warning=FALSE, echo=FALSE}

# create model parameters
model_form <- Income ~ .
model_type <- "glm"

positive_class <- "Yes"
negative_class <- "No"

trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)

# generate logistic regression model output
log_Reg_fit <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, family = binomial, trControl = trControl, metric = 'ROC')

(log_Reg_fit_Results <- log_Reg_fit$results)
confusionMatrix(log_Reg_fit, 'average')

all_predictions <- log_Reg_fit$pred

roc(all_predictions$obs, all_predictions$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc


```


####_Results from using K-fold CV with k=10 on a logistic regression model for Census Validation data ("<=50K" = Yes, ">50K" = No):_


```{r code_part_c_2, message=FALSE,  warning=FALSE, echo=FALSE}

# apply logistic regression fit to test/validation data
log_Fit_Val_Raw <- log_Reg_fit %>% predict(newdata = census_Validation_Data, type = 'raw')
log_Fit_Val_Probs <- log_Reg_fit %>% predict(newdata = census_Validation_Data, type = 'prob')

confusionMatrix(log_Fit_Val_Raw, census_Validation_Data[[response_var]], positive = positive_class)

roc(census_Validation_Data[[response_var]], log_Fit_Val_Probs[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))


```

###**d**

####_Results from using K-fold CV with k=10 on a linear discrimination model for Census Training data ("<=50K" = Yes, ">50K" = No):_



```{r code_part_d_1, message=FALSE,  warning=FALSE, echo=FALSE}

model_type <- "lda"

lda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')


# generate lda model output
(lda_fit_Results <- lda_fit_$results)
confusionMatrix(lda_fit_, 'average')

all_predictions_lda <- lda_fit_$pred

roc(all_predictions_lda$obs, all_predictions_lda$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
```

####_Results from using K-fold CV with k=10 on a linear discriminate model for Census Validation data ("<=50K" = Yes, ">50K" = No):_


```{r code_part_d_2, message=FALSE, warning=FALSE, echo=FALSE}

# apply lda fit to test/validation data
lda_fit_Val_Raw <- lda_fit_ %>% predict(newdata = census_Validation_Data, type = 'raw')
lda_fit_Val_Probs <- lda_fit_ %>% predict(newdata = census_Validation_Data, type = 'prob')

confusionMatrix(lda_fit_Val_Raw, census_Validation_Data[[response_var]], positive = positive_class)

roc(census_Validation_Data[[response_var]], lda_fit_Val_Probs[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))

```

###**e**

####_Results from using K-fold CV with k=10 on a quadratic discriminate model for Census Training data ("<=50K" = Yes, ">50K" = No):_

#####__Attempts to use QDA resulted in rank deficiency errors with the models failing to predict any of the observations as ">50 K", suggesting that the current data cannot provide for a sufficient quadratic decision boundary. Different models with less predictor variables and different values for the number of folds (K) were applied, with the only success being a model that omitted all categorical variables (only Age and WeeklyHours were used), but the performance of QDA with this model was relatively poor.__ 

```{r code_part_e_1, message=FALSE, warning=FALSE, echo=FALSE}

# attempt to identify variables that create deficient ranking errors
dataset_gr50 <- census_Data %>% filter(Income == 'Yes')
dataset_lsEq50 <- census_Data %>% filter(Income == 'No')

dataset_numerical_gr50 <- predict(dummyVars(model_form, data = dataset_gr50, fullRank = TRUE), newdata = dataset_gr50) # the dummyVars function 
dataset_numerical_lsQd50 <- predict(dummyVars(model_form, data = dataset_lsEq50, fullRank = TRUE), newdata = dataset_lsEq50)

colnames(dataset_numerical_gr50)[findLinearCombos(dataset_numerical_gr50)$remove]
colnames(dataset_numerical_lsQd50)[findLinearCombos(dataset_lsEq50)$remove]


# generate qda model output
model_type <- "qda"

# just omit all categorical variables
model_form <- Income ~ WeeklyHours + Age 

qda_fit_ <- train(as.formula(model_form) , data = census_Training_Data, method = model_type, trControl = trControl, metric = 'ROC')

(qda_fit_Results <- qda_fit_$results)
confusionMatrix(qda_fit_, 'average')

all_predictions_qda <- qda_fit_$pred

roc(all_predictions_qda$obs, all_predictions_qda$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
```

####_Results from using K-fold CV with k=10 on a quadratic discrimination model for Census Validation data ("<=50K" = Yes, ">50K" = No):_


See above for rank deficiency problems with QDA

```{r code_part_e_2, message=FALSE, warning=FALSE, echo=FALSE}

# apply qda fit to test/validation data
qda_fit_Val_Raw <- qda_fit_ %>% predict(newdata = census_Validation_Data, type = 'raw')
qda_fit_Val_Probs <- qda_fit_ %>% predict(newdata = census_Validation_Data, type = 'prob')

confusionMatrix(qda_fit_Val_Raw, census_Validation_Data[[response_var]], positive = positive_class)

roc(census_Validation_Data[[response_var]], qda_fit_Val_Probs[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))
```

###**f.**

####_Results from using K-fold CV with k=10 on a KNN (k = 3:7) model for Census Training data ("<=50K" = Yes, ">50K" = No)_

```{r code_part_f_1, message=FALSE, warning=FALSE, echo=FALSE}

# generate knn model output
model_type <- "knn"
threshold <- 0.5

tuneGrid <- expand.grid(k = 3:7)
knn_fit <- train(as.formula(model_form), data = census_Training_Data, method = model_type, trControl = trControl, tuneGrid = tuneGrid)

(knn_fit_Results <- knn_fit$results)
confusionMatrix(knn_fit, 'average')

all_predictions_knn <- knn_fit$pred

roc(all_predictions_knn$obs, all_predictions_knn$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
```


####_Results from using K-fold CV with k=10 on a KNN (k = 3:7) model for Census Validation data ("<=50K" = Yes, ">50K" = No):_


```{r code_part_f_2, message=FALSE, warning=FALSE, echo=FALSE}

# apply knn fit to test/validation data

knn_fit_Val_Raw <- knn_fit %>% predict(newdata = census_Validation_Data, type = 'raw')
knn_fit_Val_Probs <- knn_fit %>% predict(newdata = census_Validation_Data, type = 'prob')

confusionMatrix(knn_fit_Val_Raw, census_Validation_Data[[response_var]], positive = positive_class)

roc(census_Validation_Data[[response_var]], knn_fit_Val_Probs[, positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))


```


###**g**
Based solely off of training data, the logistic regression and LDA model perform comparably with AUC, sensitivity, and specificity being very close between them. The LDA model had a ~ 1.0  edge over logistic regression in terms of specificity (.53 vs .52), and if I had to choose, this would cause me to select LDA. Both QDA and KNN performed much worse.

###**h**
Both logistic regression and LDA had near-identical performances on the test data, and therefore the LDA model would once again be the slightly more desirable approach, although the difference could be considered trivial depending the emphasis for small (~ 1.0) differences in sensitivity.  
