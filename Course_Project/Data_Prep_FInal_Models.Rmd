---
title: "Data_preparation_Final_Model"
author: "Eric Troudt and Jonathan Prosser"
date: "December 10, 2019"
output: html_document
---


```{r Libraries and WD}

# --- install and load all libraries needed for the different classification models ---

## ** the glmnet package requires R 3.6.1, make sure it is installed

load.lib<-c("Hmisc", "e1071", "caret","GGally","lubridate","RANN",
"tidyverse", "pROC", "doParallel", "rpart", "rpart.plot", 
"randomForest", "xgboost", "kernlab", "glmnet", "DMwR")

install.lib<-load.lib[!load.lib %in% installed.packages()]

for(lib in install.lib) install.packages(lib,dependencies=TRUE)

sapply(load.lib,require,character=TRUE)

# set wd
tryCatch({
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
  }, error=function(cond){message(paste("cannot change working directory"))
})

# set knitr options
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)

num_cores <- detectCores()
registerDoParallel(num_cores)


```



```{r Model variable processing}

# read in model data
model_data <- read.csv("Model Dataset for Students.csv")

# are all company numbers unique? 
length(unique(model_data$Company_Number))

# read in firmographic data
firm_Data <- read.csv("Firmographic Data for Students.csv")

# relevel/recode churned variable
model_data$churned <- as.factor(model_data$churned)

model_data <- model_data %>% mutate(churned, churned = recode(churned, '0' = 'No', '1' = 'Yes'))

model_data[["churned"]] <-relevel(model_data[["churned"]], "Yes")

# How many records are shared between model data and firmographic data
common_CompanyNums <- intersect(model_data$Company_Number, firm_Data$Company_Number)
print(common_CompanyNums)

# merge model data and firmographic data on company number
merge_Data <- merge(model_data, firm_Data, by.x = 'Company_Number', by.y = 'Company_Number')

model_Variables <- c("Business_Code", "churned", "total_products", "total_transactions", "total_accounts", "total_revenue", "total_usage", "Employee_Count_Total", "Company_Number", "Number_of_Family_Members")

merge_Data_subset <- subset(merge_Data, select = model_Variables)

merge_Data_subset <- merge_Data_subset %>% mutate_all(na_if, "")

names(merge_Data_subset) <- make.names(names(merge_Data_subset))

```

```{r Model dataset imput missing values}


merge_Data_subset_NUM <- merge_Data_subset %>% select_if(is.numeric)

merge_Data_subset_nonNUM <- merge_Data_subset %>% select_if(~!is.numeric(.x))

for(Var in colnames(merge_Data_subset_nonNUM)) {
  
  merge_Data_subset_nonNUM[[Var]] <- addNA(merge_Data_subset_nonNUM[[Var]], ifany = TRUE)
  merge_Data_subset_nonNUM[[Var]] <- droplevels(merge_Data_subset_nonNUM[[Var]])
  
}

(high_Correlation <- findCorrelation(cor(merge_Data_subset_NUM), cutoff = .7))


# Use preProcess to imput missing values using KNN model, **Apply centering and scaling
pre_Imputed_model_Data <- preProcess(merge_Data_subset_NUM, method = c("bagImpute", "scale", "center"))
imputed_model_Data <- predict(pre_Imputed_model_Data, merge_Data_subset_NUM)


model_data_bagImputed<- cbind(merge_Data_subset_nonNUM, imputed_model_Data)
model_data_bagImputed <- subset(model_data_bagImputed, select = -c(Company_Creation_Date))

```



```{r Test dataset}

# read in test dataset
test_Data <- read.csv("Test Dataset for Students.csv")

merge_test_Data <- merge(test_Data, firm_Data, by.x = 'Company_Number', by.y = 'Company_Number')

# merge with firmographic data on variables selected for final model
model_Variables <- c("Business_Code.x", "total_products", "total_transactions", "total_accounts", "total_revenue", "total_usage", "Employee_Count_Total", "Company_Number", "Number_of_Family_Members")
merge_test_Data_subset <- subset(merge_test_Data, select = model_Variables)

merge_test_Data_subset <- merge_test_Data_subset %>% mutate_all(na_if, "")

names(merge_test_Data_subset)[names(merge_test_Data_subset) == "Business_Code.x"] <- "Business_Code"

names(merge_test_Data_subset) <- make.names(names(merge_test_Data_subset))


merge_test_Data_subset_NUM <- merge_test_Data_subset %>% select_if(is.numeric)

merge_test_Data_subset_nonNUM <- merge_test_Data_subset %>% select_if(~!is.numeric(.x))

# add NAs as levels to nonNumeric variables
for(Var in colnames(merge_test_Data_subset_nonNUM)) {
  
  merge_test_Data_subset_nonNUM[[Var]] <- addNA(merge_test_Data_subset_nonNUM[[Var]], ifany = TRUE)
  merge_test_Data_subset_nonNUM[[Var]] <- droplevels(merge_test_Data_subset_nonNUM[[Var]])
  
}

# center, scale, and imput missing values in test dataset using imputed prePcross object from model (training) data
imputed_real_test_Data <- predict(pre_Imputed_model_Data, merge_test_Data_subset_NUM)

real_test_Data_bagImputed<- cbind(merge_test_Data_subset_nonNUM, imputed_real_test_Data)


```

##_ Random forest and Gradient boosting trees_

```{r load and process data}


dataset <- model_data_bagImputed

target_var <- "churned"

model_form <- churned ~ . -Company_Number 

model_type <- 'rf'

positive_class <- 'Yes'
negative_class <- 'No'


trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary, search = 'grid', sampling = 'smote')

set.seed(sample(1000, 1))

trainIndex <- createDataPartition(dataset[['churned']], p = 0.7, list = FALSE)

data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]


```

###_Random forest training/test data_

```{r Random Forest model}

rf_tree_churn <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'ROC', tuneGrid = data.frame(mtry = 3))

# get training data predictions
rf_tree_churn_training_predictions <- rf_tree_churn$pred

# evaluate predictions
confusionMatrix(rf_tree_churn_training_predictions$pred, rf_tree_churn_training_predictions$obs)

roc(rf_tree_churn_training_predictions$obs, rf_tree_churn_training_predictions$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc

# predict performance on test data
rf_tree_churn_pred_raw <- rf_tree_churn %>% predict(newdata = data_test, type = 'raw')
rf_tree_churn_pred_probs <- rf_tree_churn %>% predict(newdata = data_test, type = 'prob')

# evaluate performance
confusionMatrix(rf_tree_churn_pred_raw, data_test[[target_var]], positive = positive_class)

roc(data_test[[target_var]], rf_tree_churn_pred_probs[ , positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc


```

###_Gradient boosting training/test data_

```{r Gradient boosting model}

model_type <- 'xgbTree'

trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, search = 'grid', sampling = 'smote')

tGrid <- expand.grid(nrounds = seq(10,100,10),
                   max_depth = c(1, 3, 5, 7),
                   eta = c(0.001,0.01,0.1),
                   gamma = c(0),
                   colsample_bytree = c(1),
                   min_child_weight = c(1),
                   subsample = c(1))

xgb_churn_tree <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, metric = 'ROC')


xgb_churn_tree$finalModel


xgb_churn_tree_predictions <- xgb_churn_tree$pred

# evaluate predictions
confusionMatrix(xgb_churn_tree_predictions$pred, xgb_churn_tree_predictions$obs)

roc(xgb_churn_tree_predictions$obs, xgb_churn_tree_predictions$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc

# predict performance on test data
xgb_churn_tree_pred_raw <- xgb_churn_tree %>% predict(newdata = data_test, type = 'raw')
xgb_churn_tree_pred_probs <- xgb_churn_tree %>% predict(newdata = data_test, type = 'prob')

xgb_churn_tree_pred_thresh <- factor(ifelse(xgb_churn_tree_pred_probs[, positive_class] > 0.45, positive_class, negative_class) , levels = c(negative_class, positive_class))

# chrned_model_Data <- subset(xgb_churn_tree_pred_raw, xgb_churn_tree_pred_raw == "Yes")
# non_chrned_model_Data <- subset(xgb_churn_tree_pred_raw, xgb_churn_tree_pred_raw == "No")
# 
# chrned_model_Data_thresh <- subset(xgb_churn_tree_pred_thresh, xgb_churn_tree_pred_thresh == "Yes")
# non_chrned_model_Data_thresh <- subset(xgb_churn_tree_pred_thresh, xgb_churn_tree_pred_thresh == "No")

# evaluate performance
confusionMatrix(xgb_churn_tree_pred_raw, data_test[[target_var]], positive = positive_class)

confusionMatrix(xgb_churn_tree_pred_thresh, data_test[[target_var]], positive = positive_class)

roc(data_test[[target_var]], xgb_churn_tree_pred_probs[ , positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc

varPlot_xgb_churn_tree <- varImp(xgb_churn_tree)
plot(varPlot_xgb_churn_tree)


# evaluate on actual test dataset
xgb_churn_tree_testset_raw <- xgb_churn_tree %>% predict(newdata = real_test_Data_bagImputed, type = 'raw')
xgb_churn_tree_testset_probs <- xgb_churn_tree %>% predict(newdata = real_test_Data_bagImputed, type = 'prob')

xgb_churn_tree_testset_pred <- factor(ifelse(xgb_churn_tree_testset_probs[, positive_class] > 0.45, positive_class, negative_class), levels = c(negative_class, positive_class))

final_test_data_Predictions <- cbind(real_test_Data_bagImputed, churned=xgb_churn_tree_testset_pred)
final_test_data_Predictions <- cbind(final_test_data_Predictions, probability=xgb_churn_tree_testset_probs[["Yes"]])

write.csv("final_predictions_test_Dataset.csv", x = real_test_Data_bagImputed)

# chrned_Data <- subset(xgb_churn_tree_testset_pred, xgb_churn_tree_testset_pred == "Yes")
# non_chrned_Data <- subset(xgb_churn_tree_testset_pred, xgb_churn_tree_testset_pred == "No")




```
