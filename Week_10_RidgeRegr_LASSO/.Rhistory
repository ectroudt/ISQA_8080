install.packages("rmarkdown")
```{r setup, include=FALSE}
```{r setup, include=FALSE}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
library(doParallel)
num_cores <- detectCores() #note: you can specify a smaller number if you want
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
library(doParallel)
num_cores <- detectCores() #note: you can specify a smaller number if you want
cl <- makePSOCKcluster(num_cores)
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
library(doParallel)
num_cores <- detectCores() #note: you can specify a smaller number if you want
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
tryCatch({
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}, error=function(cond){message(paste("cannot change working directory"))
})
library(RANN)
library(caret)
library(tidyverse)
cancer_Data <- read.csv("Cancer.csv")
# Inspect dataset, are all variables correctly set as factors? How are total? Are there any missing?
summary(cancer_Data)
?rpart
?RANN
library(pROC)
# Inspect dataset, are all variables correctly set as factors? How are total? Are there any missing?
summary(cancer_Data)
# Inspect dataset, are all variables correctly set as factors? How are total? Are there any missing?
str(cancer_Data)
summary(cancer_Data)
# Inspect dataset, are all variables correctly set as factors? How are total? Are there any missing?
str(cancer_Data)
View(cancer_Data)
levels(cancer_Data$bare.nuclei)[levels(cancer_Data$bare.nuclei)=="?"] <- NA
View(cancer_Data)
# Inspect dataset, are all variables correctly set as factors? How are total? Are there any missing?
str(cancer_Data)
summary(cancer_Data)
?levels
levels(cancer_Data$bare.nuclei)
cancer_Data$bare.nuclei <- addNA(cancer_Data$bare.nuclei)
levels(cancer_Data$bare.nuclei)
library(kernlab)
# Inspect dataset, are all variables correctly set as factors? How are total? Are there any missing?
str(cancer_Data)
View(cancer_Data)
cancer_Data <- cancer_Data %>% mutate(class, class = recode(class, 'Benign' = 'No', 'Malignant' = 'Yes'))
cancer_Data[['class']] <- relevel(cancer_Data[['class']], 'Yes')
# split data 80% train
set.seed(sample(1000, 1))
trainIndex <- createDataPartition(cancer_Data[['class']], p = 0.8, list = FALSE)
cancer_data_train <- cancer_Data[trainIndex,]
cancer_data_test <- cancer_Data[-trainIndex,]
library(pROC)
# set-up model params
target_var <- 'class'
# note: we can specify the formula like this. if you specify individual predictors, they have to match the column names in the dataset
model_form <- class ~ clump.thickness + uniformity.cell.size + uniformity.cell.shape + marginal.adhesion + epithelial.cell.size + bare.nuclei + bland.chromatin + normal.nucleoli + mitoses
model_type <- "svmLinear"
positive_class <- "Yes"
negative_class <- "No"
# use trainControl with cross-val
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
# set tuning parameter to 1
tGrid <- expand.grid(C = 1)
cancer_SVM <- train(as.formula(model_form), data = cancer_data_train, method = model_type, trControl = trControl, metric = 'ROC', preProc = c("center", "scale"))
cancer_SVM$finalModel
cancer_SVM
cancer_Data_Training_Predictions <- cancer_SVM$pred
confusionMatrix(cancer_Data_Training_Predictions$pred, cancer_Data_Training_Predictions$obs)
roc(cancer_Data_Training_Predictions$obs, cancer_Data_Training_Predictions$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
# split data 80% train
set.seed(sample(1000, 1))
trainIndex <- createDataPartition(cancer_Data[['class']], p = 0.7, list = FALSE)
cancer_data_train <- cancer_Data[trainIndex,]
cancer_data_test <- cancer_Data[-trainIndex,]
# use trainControl with cross-val
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
cancer_SVM <- train(as.formula(model_form), data = cancer_data_train, method = model_type, trControl = trControl, metric = 'ROC', preProc = c("center", "scale"))
cancer_SVM_Linear <- train(as.formula(model_form), data = cancer_data_train, method = model_type, trControl = trControl, metric = 'ROC', preProc = c("center", "scale"))
cancer_Data_Training_Predictions <- cancer_SVM_Linear$pred
confusionMatrix(cancer_Data_Training_Predictions$pred, cancer_Data_Training_Predictions$obs)
roc(cancer_Data_Training_Predictions$obs, cancer_Data_Training_Predictions$Yes, plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
# predict performance on test data
cancer_Data_pred_raw <- cancer_SVM_Linear %>% predict(newdata = cancer_data_test, type = 'raw')
cancer_Data_pred_probs <- cancer_SVM_Linear %>% predict(newdata = cancer_data_test, type = 'prob')
# predict performance on test data
cancer_Data_pred_raw <- cancer_SVM_Linear %>% predict(newdata = cancer_data_test, type = 'raw')
cancer_Data_pred_probs <- cancer_SVM_Linear %>% predict(newdata = cancer_data_test, type = 'prob')
# evaluate performance
confusionMatrix(cancer_Data_pred_raw, cancer_data_test[[target_var]], positive = positive_class)
roc(cancer_data_test[[target_var]], cancer_Data_pred_probs[ , positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
confusionMatrix(cancer_Data_Training_Predictions$pred, cancer_Data_Training_Predictions$obs)
# evaluate performance
confusionMatrix(cancer_Data_pred_raw, cancer_data_test[[target_var]], positive = positive_class)
set.seed(1)
# we use a grid with only one parameter (C, the cost parameter) and start by using the default value from the svm() class
tGrid <- expand.grid(C = c(0.001,0.01,0.1,0.5,1,5,10,100))
?train
cancer_SVM_Linear_CTune <- train(as.formula(model_form), data = cancer_data_train, method = model_type, trControl = trControl, tuneGrid = tGrid, metric = 'ROC', preProc = c("center", "scale"))
cancer_SVM_Linear_CTune
cancer_SVM_Linear_CTune$finalModel
# predict performance on test data
cancer_Data_CTune_pred_raw <- cancer_SVM_Linear_CTune  %>% predict(newdata = cancer_data_test, type = 'raw')
cancer_Data_CTune_pred_probs <- cancer_SVM_Linear_CTune  %>% predict(newdata = cancer_data_test, type = 'prob')
# evaluate performance
confusionMatrix(cancer_Data_CTune_pred_raw, cancer_data_test[[target_var]], positive = positive_class)
roc(cancer_data_test[[target_var]], cancer_Data_CTune_pred_probs[ , positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
library(leaps)
library(ISLR)
library(caret)
# set your working directory to the current file directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
dataset <- read.csv("College.csv")
set.seed(1)
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
dataset <- na.omit("College.csv")
set.seed(1)
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
dataset <- read.csv("College.csv")
dataset <- na.omit("College.csv")
set.seed(1)
target_var <- "Grad.Rate"
set.seed(1)
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
View(dataset)
dataset <- read.csv("College.csv")
View(dataset)
set.seed(1)
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.8, list = FALSE)
data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]
# note: we can specify the formula like this. if you specify individual predictors, they have to match the column names in the dataset
model_form <- Grad.Rate ~ .
model_type <- 'glmnet'
library(glmnet)
library(GGally)
library(caret)
library(tidyverse)
install.packages("glmnet")
# let's do a basic training-test split
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.7, list = FALSE)
data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]
library(glmnet)
model_type <- 'glmnet'
# let's do a basic training-test split
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.7, list = FALSE)
data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]
# for now, use a standard k-fold cross validation
# for now, use a standard k-fold cross validation
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE) # note: use selectionFunction = 'oneSE' to get models with fewer variables (but potentially lower performance with respect to RMSE)
tGrid <- expand.grid(alpha = c(1), lambda = 10^seq(2, -2, length = 100)) # alpha = 1 indicates LASSO
# note: as the penalty depends on the absolute size of the beta parameters, we should standardize the data first
lasso <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, tuneGrid = tGrid, preProc = c('center', 'scale'))
dataset <- na.omit(dataset)
# note: as the penalty depends on the absolute size of the beta parameters, we should standardize the data first
lasso <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, tuneGrid = tGrid, preProc = c('center', 'scale'))
View(dataset)
dataset <- ""
dataset <- read.csv("College.csv")
dataset <- na.omit(dataset)
# let's do a basic training-test split
trainIndex <- createDataPartition(dataset[[target_var]], p = 0.7, list = FALSE)
data_train <- dataset[trainIndex,]
data_test <- dataset[-trainIndex,]
# note: as the penalty depends on the absolute size of the beta parameters, we should standardize the data first
lasso <- train(as.formula(model_form), data = data_train, method = model_type, trControl = trControl, tuneGrid = tGrid, preProc = c('center', 'scale'))
library(pROC)
library(kernlab)
# set-up model params
target_var <- 'class'
# note: we can specify the formula like this. if you specify individual predictors, they have to match the column names in the dataset
model_form <- class ~ clump.thickness + uniformity.cell.size + uniformity.cell.shape + marginal.adhesion + epithelial.cell.size + bare.nuclei + bland.chromatin + normal.nucleoli + mitoses
positive_class <- "Yes"
negative_class <- "No"
# use trainControl with cross-val
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
model_type <- "svmRadial"
tGrid <- expand.grid(C = c(0.001, 0.01, 0.1, 1,5,10), sigma = c(1/nrow(data_train), 0.0001, 0.001, 0.01, 0.1, 0.5))
set.seed(1)
model_type <- "svmRadial"
trControl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary, search = 'grid')
tGrid <- expand.grid(C = c(0.001, 0.01, 0.1, 1,5,10), sigma = c(1/nrow(data_train), 0.0001, 0.001, 0.01, 0.1, 0.5))
cancer_SVM_Radial_CTune <- train(as.formula(model_form), data = cancer_data_train, method = model_type, trControl = trControl, tuneGrid = tGrid, metric = 'ROC', preProc = c("center", "scale"))
cancer_SVM_Radial_CTune$finalModel
# predict performance on test data
cancer_Data_CTune_Radial_pred_raw <- cancer_SVM_Radial_CTune  %>% predict(newdata = cancer_data_test, type = 'raw')
cancer_Data_CTune_Radial_pred_probs <- cancer_SVM_Radial_CTune  %>% predict(newdata = cancer_data_test, type = 'prob')
# evaluate performance
confusionMatrix(cancer_Data_CTune_Radial_pred_raw, cancer_data_test[[target_var]], positive = positive_class)
roc(cancer_data_test[[target_var]], cancer_Data_CTune_Radial_pred_probs[ , positive_class], plot = TRUE, print.auc = TRUE, legacy.axes = TRUE, levels = c(negative_class, positive_class))$auc
View(cancer_SVM_Radial_CTune)
